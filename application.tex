\section{Applications}
\label{sec:application}


\begin{figure*}[t!]
	\begin{minipage}{.31\textwidth}
    	\centering
		\includegraphics[width=\textwidth]{gnuplot/reorder_receiver.eps}
		\caption{Reordering CPU and memory overhead on hosts.}
		\label{fig:reorder-overhead}

		\centering
		\includegraphics[width=\textwidth]{gnuplot/reordering_delay.eps}
		\caption{Reordering delay with clock synchronization methods and background traffic (BG).}
		\label{fig:clock-sync}
	\end{minipage}
	\hspace{0.01\textwidth}
    \begin{minipage}{.31\textwidth}
      \centering
      \subfloat[Throughput.\label{fig:loss-throughput}]
      {\includegraphics[width=\textwidth]{gnuplot/loss_tput.eps}}
      \vspace{0.01\textwidth}
      \subfloat[Latency.\label{fig:loss-latency}]
      {\includegraphics[width=\textwidth]{gnuplot/loss_latency.eps}}
      \caption{Comparison of reliable \sys and application-layer transaction rollback.}
      \label{fig:ycsb-loss}
    \end{minipage}
    \hspace{0.01\textwidth}
    \begin{minipage}{.31\textwidth}
    	\centering
		\includegraphics[width=\textwidth]{gnuplot/ycsb.eps}
		\caption{Throughput and latency of YCSB+T transactional key-value stores.}
		\label{fig:ycsb}

		\centering
		\includegraphics[width=\textwidth]{gnuplot/multishard.eps}
		\caption{Throughput of multiple keys per transaction.}
		\label{fig:multishard}
    \end{minipage}
\end{figure*}


\subsection{Atomic Multi-Site Operations}
\label{subsec:eval-kvs}



As a case study, we evaluate the performance of atomic multi-site operations using YCSB+T~\cite{dey2014ycsbt} skewed workload.
For simplicity, we assume each site is an in-memory key-value store.
Replication, logging and durability are out of scope of this paper.
Clients scatter operations to sites and gather responses.

%As a case study, we study the transaction performance of transactional key-value stores under YCSB+T~\cite{dey2014ycsbt} skewed workload.
%For the sake of simplicity, we focus on single round-trip (one-shot) transactions. Single-node transactions can be serialized on the server at the current timestamp, thus do not need \sys. General transactions with predetermined read and write sets can be reduced to single round-trip transactions, by sending the read and write set to each shard via \sys. During further execution of a general transaction, each shard can track the dependency and execute the read and write operations in timestamp order.

We use the second testbed (Arista switches).
Figure~\ref{fig:ycsb} shows the throughput and latency, where each atomic operation accesses two randomly chosen objects.
\sys achieves 6.4~M transactions per server (0.8~M transactions per core) with 40~$\mu$s latency, close to an non-atomic system (NonTX) where operations are scattered directly without ordering.
The throughput gap between \sys and NonTX is mainly due to CPU processing of reordering messages.
The latency gap originates from the reordering delay.
In comparison, the throughput of locks is limited by RTT.
The throughput of centralized coordination is bottlenecked by the coordinator.

Figure~\ref{fig:multishard} compares the throughput \RED{what throughput, single server? single core?} with an increasing number of keys per atomic scattering.
Now we consider other timestamp-based methods.
With more keys per scattering, the chance of transaction abort increases, because reordering in any one of the keys would cause transaction abort.

%Figure~\ref{fig:multishard} compares the throughput of TOMS, OCC, lock-based and non-transactional key-value stores.
%Each transaction accesses 8 unique remote objects.
%If the 8 objects are on two shards, OCC has 50\% chance of transaction abort, so the throughput is 50\% of non-transactional systems. When the objects are distributed on more shards, OCC has an exponentially higher chance of abort (Sec.~\ref{sec:toms}). The throughputs of other systems are unrelated to number of shards. \sys has 10x transaction throughput than lock-based systems, as well as OCC systems with more than 4 shards per transaction. The throughput of \sys is close to the theoretical bound of non-transactional system.

Finally, we study the efficiency of our loss detection and recovery mechanism in Sec.~\ref{sec:lossy}.
Assume 1/10 of the transactions have conflicts, \textit{i.e.}, if a transaction is rollbacked, 1/10 of uncommitted transactions also need rollback.
Figure~\ref{fig:ycsb-loss} simulate the transaction throughput and latency under different loss ratios.
With reliable \sys, packet loss is transparent to applications, and the transaction throughput is approximately the network goodput.
If \sys does not handle packet loss, the transaction processing application needs to rollback all uncommitted transactions in conflict with the lost transaction, so a significant portion of CPU power and network bandwidth is wasted.

On the latency side, reliable \sys adds one RTT of latency to derive the delivery barrier.
If any packet is lost, reliable \sys needs to wait for an additional RTT to retransmit the packet.
When packet loss is rare, as in most data center networks, handling loss by applications provides lower latency.
Under high packet loss probability, however, handling losses in \sys is better.

\iffalse
\begin{figure}[t]
\centering
\includegraphics[width=0.3\textwidth]{images/fixme.pdf}
\caption{[Testbed] Comparing YCSB+T throughput on inter-DC WAN.}
\vspace{-10pt}
\label{fig:ycsb-inter-dc}
\end{figure}

Figure~\ref{fig:ycsb-inter-dc} compares the throughput of cross-datacenter workload of TOMS and lock-based.
\fi

\subsection{Concurrency Control in Transactions}
\label{subsec:eval-transactions}

Now we apply \sys to concurrent control in one-shot transactions.
We benchmark New-Order and Payment transactions in TPC-C~\cite{tpcc}.


\begin{figure*}[t]
\centering
	\subfloat[New-Order 50\%, Payment 50\%.\label{fig:tpc-c-new-order}]
	{\includegraphics[width=.3\textwidth]{gnuplot/tpcc-combined.eps}}
	\hspace{0.02\textwidth}
	\subfloat[Payment 100\%.\label{fig:tpc-c-payment}]
	{\includegraphics[width=.3\textwidth]{gnuplot/tpcc-payment.eps}}
    \hspace{0.02\textwidth}
    \subfloat[New-Order 100\%.\label{fig:tpc-c-payment}]
	{\includegraphics[width=.3\textwidth]{gnuplot/tpcc-neworder.eps}}
\caption{TPC-C benchmark with 4 warehouses.}
\label{fig:tpc-c}
\end{figure*}

Performance with 4 warehouses.
X axis: number of concurrent clients.~\cite{yu2014staring}.
For Payment transaction, throughput cannot scale with number of concurrent clients, because all payment transactions updates a shared field.

With 1024 districts.
OCC peak at X due to timestamp assignment.~\cite{yu2014staring} Ours can still scale because no centralized timestamp assignment.

\subsection{Serializable Log Replication}
\label{subsec:eval-replication}

Compare latency and throughput of \sys with network sequencer (control path) and centralized coordinator (data path).
Just list some numbers in one paragraph.
\RED{Bojie should fill in the numbers.}



%\subsection{Multiple Round-Trip Transactions}

%TPC-C transaction benchmark

%Compare transaction throughput with Eris, TAPIR, DrTM+R (lock based), OCC and theoretical optimal (non-transactional)

%\subsection{Coordination-Free Causal Ordering}

%Our barrier timestamp is guaranteed to be lower than data timestamp, so if we send a message A after receiving a message B, A has higher timestamp than B. This can ensure ordering of events in a distributed system. For example, client A sends a command W to write database D, then sends a message to client B. When B receives the message from A, it sends command R to read database D. In our system, the database read command R is guaranteed to be processed after the write command W, so no additional synchronization is needed.