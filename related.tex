\section{Related Work}
\label{sec:related}

\parab{Causal and totally ordered communication (CATOCS).}
Critics~\cite{cheriton1994understanding} and proponents~\cite{birman1994response,van1994bother} of CATOCS have long discussed the pros and cons of such a primitive.
\sys provides a scattering primitive with restricted atomicity, and achieves scalability with in-network computation that incurs little overhead, thus removing two criticisms (can't say together and efficiency).
Scattering also enables atomic access to a portion (rather than all) of shards.
Although \sys is not a panacea for ordering problems, \revise{\textit{e.g.}, it is not sufficient to support serializable general transactions}, it shows effectiveness in applications in Sec.\ref{sec:application}.

%Since the dawn of distributed system research~\cite{lamport1978time}, there has been extensive research in total order communication, mostly providing a multicast or broadcast primitive.
There has been extensive research on total order broadcast~\cite{defago2004total}.
One line of work leverages logically centralized coordination, \textit{e.g.}, centralized sequencers~\cite{birman1991lightweight,kaashoek1996evaluation,eris} or a token passed among senders or receivers~\cite{rajagopalan1989token,moser1996totem,kim1997total,ekwall2004token}.
As a result, it is challenging to scale the system.
Another line of work buffers messages at receivers and builds a causality graph at receivers~\cite{lamport1978time,peterson1989preserving}, merges streams deterministically or achieves agreement among receivers \cite{birman1987exploiting,chandra1996unreliable,pedone1998optimistic,junqueira2011zab}.
This causes extra delay and network bandwidth overhead.
A third line of work assumes a synchronous network~\cite{lamport1984using}, but lossy links and bad clocks with high skew violate correctness. In \sys{}, such components will slow down the whole system but correctness is preserved. In \sys{}, such components should be detected, and affected hosts can use healthy nodes as proxy, where timestamps are assigned at the proxy.

A fourth line of work, tree-based algorithms, addresses the trade-off between efficiency and scalability~\cite{rodrigues1998scalable}.
At each non-leaf node, multiple ordered streams of packets are merged into one ordered stream, called \textit{deterministic merge}~\cite{hadzilacos1994modular,aguilera2000efficient}.
\revise{This usage of timestamp and ordering is similar to \sys{} while applied to other fields such as network switches~\cite{network-switch-patent}, multi-core processors~\cite{kaestle2016machine}, and sensor networks~\cite{chakraborty2011reliable}.}
%However, centralized bottleneck still appears at the root of a broadcast tree, where all traffic between two sub-trees need to pass through.
However, it is not practical to implement deterministic merge in commodity network switches.
First, switches in data centers have small per-port buffer size~\cite{bai2017congestion}.
Second, commodity switches cannot reorder packets in priority queues according to per-packet metadata~\cite{sivaraman2016programmable,jin2018netchain}.


\parab{Network and system co-design.}
Recent years witness a trend of co-designing distributed systems with programmable network switches.
Mostly Ordered Multicast~\cite{ports2015designing} and NO-Paxos~\cite{li2016just} use a switch as a centralized sequencer or serialization point to achieve totally ordered broadcast.
Different from broadcast, \sys{} provides a scattering primitive and a scalable implementation.
Eris~\cite{eris} proposes in-network concurrency control using switch as a centralized sequencer.
NetChain~\cite{jin2018netchain}, NetLock~\cite{yu2020netlock}, NetPaxos~\cite{dang2015netpaxos}, and DAIET~\cite{sapio2017network} offload important distributed middlewares to programmable switches.
Omnisequencing~\cite{michael2018towards} finds out that DCN topology can provide causal (but not total order) message delivery.
%\sys{} is inspired by Lamport's idea that the absence of a message can carry information in a synchronous network~\cite{lamport1984using}. %We achieve synchrony via timestamp barriers.

%Using Time instead of Timeout for fault-tolerant distributed systems.

%Lamport: the absence of a message can carry information.

%Distributed lock management with RDMA. decentralization without starvation.

%\RED{NetChain. Section 4.3 of the paper shows that comparing sequence numbers for each item (network port) is implementable in network switches. Furthermore, the use of sequence number to validate serialization order implies that reordering packets buffered in a network switch is not possible. This agrees with the design principle of TOMS.}



%\parab{Transaction commitment protocols.} 2PC, 3PC, Paxos.

%\parab{Hardware accelerated concurrency control.}
%There is rich literature in achieving strong consistency efficiently in distributed systems.%~\cite{lloyd2011don,lloyd2013stronger,mu2014extracting,zhang2016operation,lu2015existential,ajoux2015challenges,mu2016consolidating,lu2016snow,kallman2008h,zhang2015building}.
%For lock-based concurrency control, most recent works~\cite{dragojevic2014farm,kalia2016fasst,kaminsky2016design,dragojevic2015no} use RDMA to offload network processing and lock handling to NICs.
%To reduce transaction abort rate in timestamp-based OCC, the key idea is to process transactions in monotonic timestamp order.
%Mostly-Ordered Multicast~\cite{ports2015designing}, NOPaxos~\cite{li2016just}, Eris~\cite{eris} and NetChain~\cite{jin2018netchain} use network switches to ensure total ordering of message timestamps.

%\textbf{In-network computation.}
%In-network computation flourishes with the trend of programmable switches~\cite{lu2011serverswitch,tofino,bosshart2013forwarding} and NICs~\cite{kaufmann2016high,clicknp}.
%Programmable switches can cache frequently accessed data in key-value stores~\cite{li2016fast,netcache-sosp17,kv-direct} and network infrastructure services~\cite{fayazbakhsh2013less,liu2017incbricks,miao2017silkroad}.
%The wide visibility of programmable switches make them a better place to implement consensus protocols~\cite{dang2016network,dang2016paxos,dang2015netpaxos}.

\iffalse
\parab{Bounded delay.}A network with bounded delay can improve reordering delay of \sys. Achieving bounded delay requires the queue length to be bounded and the network to be lossless. Recent years see progress in lossless data center networks~\cite{calder2013don,cheng2014catch,handley2017re} and fast detection of packet losses~\cite{li2016lossradar}.
Bounded queuing delay is also enabled by centralized controller~\cite{perry2015fastpass} or end-to-end flow control~\cite{gao2015phost,cho2017credit}.
\fi



\iffalse


Spanner: Google’s globally distributed database (OSDI\'12)~\cite{corbett2013spanner},
Linearizabiliy: A correctness condition for concurrent objects (ACM Trans, 1990),~\cite{herlihy1990linearizability}
Low overhead concurrency control for partitioned main memory databases (SIGMOD\'10),~\cite{jones2010low}
H-Store: a highperformance,
distributed main memory transaction processing
system (VLDB\'08),~\cite{kallman2008h}
MDCC: multi-data center consistency (Eurosys\'13),~\cite{kraska2013mdcc}
Calvin: Fast distributed transactions for
partitioned database systems~\cite{thomson2012calvin}
Fast Distributed Transactions and Strongly Consistent Replication
for OLTP Database Systems (Calvin'14),~\cite{thomson2014fast}
Transaction Processing Performance Council. TPC
Benchmark C.~\cite{council2005transaction}

Receiver time synchronization with MIN:
Fine-Grained Network Time Synchronization using Reference
Broadcasts, OSDI'02.~\cite{elson2002fine}
Virtual time, 1985.~\cite{jefferson1985virtual}


Zookeeper: A simple totally ordered broadcast protocol

Raft

The End of an Architectural Era
(It’s Time for a Complete Rewrite)

OLTP through the looking glass, and what we found there

The VoltDB Main Memory DBMS

Viewstamped Replication [19] and Raft [20], ZAB [16], and Multi-Paxos [3]

In modern distributed databases, consensus is often used as the basis for replication, to ensure replicas apply updates in the same order, an instance of state-machine replication (see Schneider’s tutorial Schneider, F.B. Implementing fault-tolerant services using the state machine approach: A tutorial).

We solve locking (2PL): Jim Gray, Raymond A. Lorie, Gianfranco R. Putzolu, Irving L. Traiger. Granularity of Locks and Degrees of Consistency in a Shared Data Base. , IBM, September, 1975.

Stonebraker: My current best guess is that nobody will use traditional two phase locking. Techniques based on timestamp ordering or multiple versions are likely to prevail. The third paper in this section discusses Hekaton, which implements a state-of-the art MVCC scheme.

Cristian Diaconu, Craig Freedman, Erik Ismert, Per-Ake Larson, Pravin Mittal, Ryan Stonecipher, Nitin Verma, Mike Zwilling. Hekaton: SQL Server's Memory-optimized OLTP Engine. SIGMOD, 2013.



Orthogonal to 2PC (Two phase commit):
Lampson, B. and Sturgis, H. Crash recovery in a distributed data storage system. Xerox PARC Technical Report. 1979.

On Interprocess Communication, Lamport, regular register

Linearizability: A Correctness Condition for
Concurrent Objects, Jennette Wing

The serializability of concurrent database updates, 1979

Useless Actions Make a Difference:
Strict Serializability of Database Updates

Berkeley algorithm (average):
The accuracy of the clock synchronization achieved by TEMPO in Berkeley UNIX 4.3BSD

SRM (Scalable Reliable Multicast):
A Reliable Multicast Framework for Light-weight Sessions
and Application Level Framing

Scalable multicast:
Hierarchical feedback 

The Implementation of Reliable Distributed Multiprocess Systems

control~\cite{tanenbaum2007distributed}


Physical clock synchronization: Fault-Tolerant Clock Synchronization, 
Optimal clock synchronization

Sync to fastest clock: Clock synchronization algorithm for address independent networks (patent),
\fi
