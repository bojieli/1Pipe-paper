\section{Related Work}
\label{sec:related}

\parab{Total order communication.}
Since the dawn of distributed system research~\cite{lamport1978time}, there has been extensive research in total order communication, mostly providing a multicast or broadcast primitive.
One line of work leverages logically centralized coordination, \textit{e.g.}, centralized sequencers~\cite{eris} or a token passed among senders or receivers~\cite{rajagopalan1989token,kim1997total,ekwall2004token}.
As a result, it is challenging to scale the system.
Another line of work uses fully distributed coordination, \textit{e.g.}, exchange timestamps among receivers before they start to process messages~\cite{lamport1978time}, or aggregate history during message delivery~\cite{chandra1996unreliable}.
This causes extra network communication overhead and delay, thus degrading system efficiency.
A third line of work assumes a synchronous network, \textit{e.g.}, the block generation interval in Bitcoin~\cite{nakamoto2008bitcoin} is designed to be higher than the maximum delay among hosts.
However, in data center systems, waiting for worst-case delay leads to unacceptable latency.

Critics and proponents of causal and totally ordered communication (CATOCS) have long discussed the pros and cons of such a primitive~\cite{cheriton1994understanding,birman1994response,van1994bother}. 
\sys achieves scalability with in-network computation and incurs little overhead, thus removing one of the biggest criticisms of this primitive.

\parab{Network and system co-design.}
Recent years witness a trend of co-designing distributed systems with programmable network switches.
Mostly Ordered Multicast~\cite{ports2015designing} and NOPaxos~\cite{li2016just} use a switch as a centralized sequencer or serialization point to achieve fast consensus.
However, consensus does not imply FIFO ordering~\cite{junqueira2011zab}.
Eris~\cite{eris} proposes in-network concurrency control using switch as a centralized sequencer.
NetChain~\cite{jin2018netchain} uses a chain of switches for scale-free sub-RTT coordination.
\sys{} is inspired by Lamport's idea that the absence of a message can carry information in a synchronous network~\cite{lamport1984using}. We achieve synchrony via timestamp barriers.

%Using Time instead of Timeout for fault-tolerant distributed systems.

%Lamport: the absence of a message can carry information.

%Distributed lock management with RDMA. decentralization without starvation.

%\RED{NetChain. Section 4.3 of the paper shows that comparing sequence numbers for each item (network port) is implementable in network switches. Furthermore, the use of sequence number to validate serialization order implies that reordering packets buffered in a network switch is not possible. This agrees with the design principle of TOMS.}


\parab{Tree-based total order.}
Tree-based algorithms for total order communication address the trade-off between efficiency and scalability~\cite{rodrigues1998scalable}, which is used in multi-core processors~\cite{kaestle2016machine} and sensor networks~\cite{chakraborty2011reliable}.
At each non-leaf node, multiple ordered streams of packets are merged into one ordered stream, called \textit{deterministic merge}~\cite{hadzilacos1994modular, aguilera2000efficient}.
However, centralized bottleneck still appears at the root of a broadcast tree, where all traffic between two sub-trees need to pass through.
In addition, it is not practical to implement deterministic merge in commodity network switches.
First, switches in data centers have small per-port buffer size~\cite{bai2017congestion}.
Second, commodity switches do not support scheduling queues according to per-packet metadata~\cite{sivaraman2016programmable,jin2018netchain}.


%\parab{Transaction commitment protocols.} 2PC, 3PC, Paxos.

%\parab{Hardware accelerated concurrency control.}
%There is rich literature in achieving strong consistency efficiently in distributed systems.%~\cite{lloyd2011don,lloyd2013stronger,mu2014extracting,zhang2016operation,lu2015existential,ajoux2015challenges,mu2016consolidating,lu2016snow,kallman2008h,zhang2015building}.
%For lock-based concurrency control, most recent works~\cite{dragojevic2014farm,kalia2016fasst,kaminsky2016design,dragojevic2015no} use RDMA to offload network processing and lock handling to NICs.
%To reduce transaction abort rate in timestamp-based OCC, the key idea is to process transactions in monotonic timestamp order.
%Mostly-Ordered Multicast~\cite{ports2015designing}, NOPaxos~\cite{li2016just}, Eris~\cite{eris} and NetChain~\cite{jin2018netchain} use network switches to ensure total ordering of message timestamps.

%\textbf{In-network computation.}
%In-network computation flourishes with the trend of programmable switches~\cite{lu2011serverswitch,tofino,bosshart2013forwarding} and NICs~\cite{kaufmann2016high,clicknp}.
%Programmable switches can cache frequently accessed data in key-value stores~\cite{li2016fast,netcache-sosp17,kv-direct} and network infrastructure services~\cite{fayazbakhsh2013less,liu2017incbricks,miao2017silkroad}.
%The wide visibility of programmable switches make them a better place to implement consensus protocols~\cite{dang2016network,dang2016paxos,dang2015netpaxos}.

\iffalse
\parab{Bounded delay.}A network with bounded delay can improve reordering delay of \sys. Achieving bounded delay requires the queue length to be bounded and the network to be lossless. Recent years see progress in lossless data center networks~\cite{calder2013don,cheng2014catch,handley2017re} and fast detection of packet losses~\cite{li2016lossradar}.
Bounded queuing delay is also enabled by centralized controller~\cite{perry2015fastpass} or end-to-end flow control~\cite{gao2015phost,cho2017credit}.
\fi



\iffalse


Spanner: Google’s globally distributed database (OSDI\'12)~\cite{corbett2013spanner},
Linearizabiliy: A correctness condition for concurrent objects (ACM Trans, 1990),~\cite{herlihy1990linearizability}
Low overhead concurrency control for partitioned main memory databases (SIGMOD\'10),~\cite{jones2010low}
H-Store: a highperformance,
distributed main memory transaction processing
system (VLDB\'08),~\cite{kallman2008h}
MDCC: multi-data center consistency (Eurosys\'13),~\cite{kraska2013mdcc}
Calvin: Fast distributed transactions for
partitioned database systems~\cite{thomson2012calvin}
Fast Distributed Transactions and Strongly Consistent Replication
for OLTP Database Systems (Calvin'14),~\cite{thomson2014fast}
Transaction Processing Performance Council. TPC
Benchmark C.~\cite{council2005transaction}

Receiver time synchronization with MIN:
Fine-Grained Network Time Synchronization using Reference
Broadcasts, OSDI'02.~\cite{elson2002fine}
Virtual time, 1985.~\cite{jefferson1985virtual}


Zookeeper: A simple totally ordered broadcast protocol

Raft

The End of an Architectural Era
(It’s Time for a Complete Rewrite)

OLTP through the looking glass, and what we found there

The VoltDB Main Memory DBMS

Viewstamped Replication [19] and Raft [20], ZAB [16], and Multi-Paxos [3]

In modern distributed databases, consensus is often used as the basis for replication, to ensure replicas apply updates in the same order, an instance of state-machine replication (see Schneider’s tutorial Schneider, F.B. Implementing fault-tolerant services using the state machine approach: A tutorial).

We solve locking (2PL): Jim Gray, Raymond A. Lorie, Gianfranco R. Putzolu, Irving L. Traiger. Granularity of Locks and Degrees of Consistency in a Shared Data Base. , IBM, September, 1975.

Stonebraker: My current best guess is that nobody will use traditional two phase locking. Techniques based on timestamp ordering or multiple versions are likely to prevail. The third paper in this section discusses Hekaton, which implements a state-of-the art MVCC scheme.

Cristian Diaconu, Craig Freedman, Erik Ismert, Per-Ake Larson, Pravin Mittal, Ryan Stonecipher, Nitin Verma, Mike Zwilling. Hekaton: SQL Server's Memory-optimized OLTP Engine. SIGMOD, 2013.



Orthogonal to 2PC (Two phase commit):
Lampson, B. and Sturgis, H. Crash recovery in a distributed data storage system. Xerox PARC Technical Report. 1979.

On Interprocess Communication, Lamport, regular register

Linearizability: A Correctness Condition for
Concurrent Objects, Jennette Wing

The serializability of concurrent database updates, 1979

Useless Actions Make a Difference:
Strict Serializability of Database Updates

Berkeley algorithm (average):
The accuracy of the clock synchronization achieved by TEMPO in Berkeley UNIX 4.3BSD

SRM (Scalable Reliable Multicast):
A Reliable Multicast Framework for Light-weight Sessions
and Application Level Framing

Scalable multicast:
Hierarchical feedback 

The Implementation of Reliable Distributed Multiprocess Systems

control~\cite{tanenbaum2007distributed}


Physical clock synchronization: Fault-Tolerant Clock Synchronization, 
Optimal clock synchronization

Sync to fastest clock: Clock synchronization algorithm for address independent networks (patent),
\fi
