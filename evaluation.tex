\section{Evaluation}
\label{sec:evaluation}

\begin{figure*}[t]
        \begin{minipage}[t]{.32\textwidth}
                \centering
                \subfloat[Throughput]{
                        \includegraphics[width=\textwidth]{gnuplot/total_order.eps}
                }
                \newline
                \subfloat[Latency]{
                        \includegraphics[width=\textwidth]{gnuplot/total_order_lat.eps}
                }
                \caption{Scalability comparison of total order algorithms.}
                \label{fig:total-order}
        \end{minipage}
        \hspace{0.01\textwidth}
        \begin{minipage}[t]{.32\textwidth}
                \centering
                \subfloat[Reconfigurable chip, switch CPU and end hosts.\label{fig:reorder-testbed}]
                {\includegraphics[width=\textwidth]{gnuplot/reorder_testbed.eps}}
                \newline
                \subfloat[Simulation with different end-host delays.
                T-R\textit{i} shows minimax clock synchronization, P-R\textit{i} shows physical clock synchronization.\label{fig:reorder-simulation}]
                {\includegraphics[width=\textwidth]{gnuplot/reorder_simulation.eps}}
                \caption{Average reordering delay.}
                \label{fig:reorder-delay}
        \end{minipage}
        \hspace{0.01\textwidth}
        \begin{minipage}[t]{.32\textwidth}
                \centering
                \subfloat[Network bandwidth overhead.\label{fig:network-overhead}]
                {\includegraphics[width=\textwidth]{gnuplot/beacon_network_overhead.eps}}
                \newline
                \subfloat[CPU processing overhead.\label{fig:cpu-overhead}]
                {\includegraphics[width=\textwidth]{gnuplot/beacon_cpu_overhead.eps}}
                \vspace{-10pt}
                \caption{
                        Beacon overhead under different beacon intervals and network throughput.
                        CPU processing overhead for Arista switch is extrapolated.
                }
                \label{fig:overhead}
        \end{minipage}
%\vspace{-1.6em}
\vspace{-15pt}
\end{figure*}

%We prove the correctness and liveness of \sys in Appendix~\ref{appx:proof}.
We summarize the key findings as follows:

%Our evaluation centers around four aspects:

\parab{Scalable throughput.}\sys achieves throughput close to the underlying hardware limit, and scales linearly to 1K hosts.

\parab{Low latency.}With reconfigurable chip, \sys adds less than 5$\mu$s to one-way delay for 1K hosts. With switch CPU, \sys adds less than 15$\mu$s delay.

\parab{Low network and CPU overhead.}\sys's beacons only consume 0.1\% link capacity. At switches, beacon packets can be processed by a CPU core. At hosts, reordering computation consumes less than 10\% of a core.

\parab{Resilient to background traffic.}\sys is adaptive to delay variance in the network by re-synchronizing clocks for every beacon interval. Hence, \sys can share the network fairly with background TCP traffic.

\subsection{Methodology}
\label{sec:testbed}

We build two testbeds in two data centers.
The first one has 3 servers connected to a Barefoot Tofino 100G switch~\cite{tofino}.
The second one has 8 servers and 4 Arista 7060CX 100G switches~\cite{arista}, forming a fat-tree topology.
Each server has two Xeon E5 CPUs and one Mellanox ConnectX-4 NIC running RoCEv2~\cite{infinibandrocev2}.

%We use a testbed of 8 Dell R720/R730 servers and 10 Arista 100G Ethernet switches~\cite{arista} to benchmark the efficiency of \sys. %The topology is similar to Figure~\ref{fig:dcn}.
%Each server is equipped with two Xeon E5 CPUs and one Mellanox ConnectX-4 NIC. Because we have not found a low latency interface in the switch OS to process beacon packets, we connect one Dell R720 server with Mellanox ConnectX-3 NICs to each of the switches to mimic the switch CPU. %Since the bottleneck in our benchmarks is the CPU instead of the network, there is unlikely to be congestion loss.
%Performance using data-plane programmable switches are expected to be better, but not evaluated because we do not have such a switch.

In small-scale experiments (1$\sim$8 hosts), each server uses a pair of dedicated cores for sending and receiving, respectively.
For experiments with 16$\sim$128 hosts, we use each physical server as 16 logical servers. Each logical server has a pair of dedicated cores, maintains timestamps independently and communicates via separate RDMA queue pairs.
To evaluate datacenter-scale scalability ($\ge$256 hosts), we build a simple simulator with random link one-way delays from $U(0\mu s,10\mu s)$.
We choose beacon interval to be 10~$\mu$s.


\iffalse
\begin{table}[t]
\centering
\scalebox{0.85}{
\begin{tabular}{|l|r|r|r|r|r|r|}
\hline
Servers & \multicolumn{3}{c|}{Num switches} & \multicolumn{3}{c|}{Num downlinks} \\
\hline
Host & ToR & Leaf & Core & ToR & Leaf & Core \\
\hline
2        & 1 & 0 & 0 & 2 & 0 & 0 \\
\hline
4    & 2 & 2 & 0 & 2 & 2 & 0 \\
\hline
8    & 4 & 4 & 2 & 2 & 2 & 4 \\
\hline
64    & 8 & 4 & 2 & 8 & 4 & 4 \\
\hline
1024  & 64 & 16 & 8 & 16 & 16 & 16 \\
\hline
\end{tabular}
\vspace{-10pt}
}
\caption{Network topologies for evaluation.}
\label{tab:eval-topology}
\end{table}

We evaluate 5 different system scales in Table~\ref{tab:eval-topology}: testbed consisting of one to three layers of network switches, as well as simulation of three-layer fat-tree topology with 64 or 1024 hosts.
Topologies in Table~\ref{tab:eval-topology}.
For large-scale experiments, we use NS-3~\cite{henderson2008network} for simulation.
The link bandwidth, link delay and processing delay are extracted from three-layer testbed experiments.
To make simulations faster, each sender only sends one message to a randomly chosen receiver.
\fi

\begin{figure*}[t!]
	\begin{minipage}{.31\textwidth}
    	\centering
		\includegraphics[width=\textwidth]{gnuplot/reorder_receiver.eps}
		\caption{Reordering CPU and memory overhead on hosts.}
		\label{fig:reorder-overhead}

		\centering
		\includegraphics[width=\textwidth]{gnuplot/reordering_delay.eps}
		\caption{Reordering delay with clock synchronization methods and background traffic (BG).}
		\label{fig:clock-sync}
	\end{minipage}
    \hspace{0.01\textwidth}
    \begin{minipage}{.31\textwidth}
    	\centering
		\includegraphics[width=\textwidth]{gnuplot/ycsb.eps}
		\caption{Throughput and latency of YCSB+T transactional key-value stores.}
		\label{fig:ycsb}

		\centering
		\includegraphics[width=\textwidth]{gnuplot/multishard.eps}
		\caption{Throughput of multiple keys per transaction.}
		\label{fig:multishard}
    \end{minipage}
	\hspace{0.01\textwidth}
	\begin{minipage}{.31\textwidth}
		\centering
		\subfloat[Throughput.\label{fig:loss-throughput}]
		{\includegraphics[width=\textwidth]{gnuplot/loss_tput.eps}}
		\vspace{0.01\textwidth}
		\subfloat[Latency.\label{fig:loss-latency}]
		{\includegraphics[width=\textwidth]{gnuplot/loss_latency.eps}}
		\caption{Comparison of reliable \sys and application-layer transaction rollback.}
		\label{fig:ycsb-loss}
	\end{minipage}
\vspace{-15pt}
\end{figure*}

\subsection{Results}
\parab{Scalability.}\label{subsubsec:scalability}
Figure~\ref{fig:total-order} compares the scalability of \sys with other total order multicast algorithms using centralized sequencer~\cite{eris,kaminsky2016design}, token~\cite{rajagopalan1989token}, ISIS~\cite{birman1985replication} and Lamport timestamp~\cite{lamport1978time}. The throughput of \sys is limited by CPU processing and RDMA messaging rate (8~M messages per second per core).
It scales almost linearly to 128 hosts in the testbed and 1K hosts in simulation. In contrast, the sequencer is a central bottleneck and introduces extra network delays. In timestamp-based ordering, a receiver waits for timestamps from other receivers before delivery, increasing delivery delay and network overhead. Note that serializable log replication to multiple replicas can be implemented by scattering each log entry to all replicas. Hence, \sys can achieve scalable and efficient log replication.

%Although recent advances in RDMA~\cite{kaminsky2016design} and programmable switches~\cite{eris} make sequencers fast, it is still a central bottleneck and introduces extra network delay.






\parab{Reordering delay.}Figure~\ref{fig:reorder-testbed} shows the average reordering delay of \sys.
Reordering delay is the interval from message arrival to delivery. As expected, reconfigurable chip delivers the smallest reordering delay. In contrast, end host representatives introduces extra forwarding delay from the switch to the end host. As the number of network layers increases, the reordering delay also increases, due to per-hop beacon processing delay.
%\RED{The beacon interval is not amplified by network layers, because the algorithm synchronizes beacon arrival times. doesn't make sense}
For the three-layer topology, a larger number of hosts slightly increases reordering delay, because a switch with higher fan-out is likely to have more skew in beacon synchronization.

Figure~\ref{fig:reorder-simulation} shows how minimax clock synchronization adapts to imbalanced link or host delays. We simulate two senders and two receivers connected via 4 links. When one sender $S_1$ has 10~$\mu$s more delay due to background traffic, minimax clock synchronization adjusts the sender's timestamp to preserve minimal reordering delay, while physical clock synchronization does not account for the different sender delays. When one network link $S_2 \rightarrow R_2$ has 100~$\mu$s more delay, due to multi-path, both synchronization mechanisms have more reordering delay. When two links $S_2 \rightarrow R_1$ and $S_2 \rightarrow R_2$ both have 10~$\mu$s more delay, minimax clock synchronization can compensate the change.



%\begin{figure}[t]
%\centering
%\includegraphics[width=0.48\textwidth]{images/fixme.pdf}
%\caption{CDF of end-to-end delay and reordering delay.}
%\label{fig:cdf-delay}
%\end{figure}



\parab{Network overhead.}As shown in Figure~\ref{fig:network-overhead}, with the high link capacity and a reasonable beacon generation interval, beacon traffic is a tiny portion of available link capacity. Because beacons are hop-by-hop, the overhead is only determined by the beacon interval and does not change with the system scale.

\parab{CPU overhead.}The CPU overhead of \sys has two parts: beacon processing at switches and reordering at receivers. Figure~\ref{fig:cpu-overhead} shows the number of cores required for beacon processing of a 32-port switch. The Arista switch has 4 CPU cores. The raw packet processing capacity of a switch CPU core is 1/3 of a Xeon E5 CPU core. If we can bypass the kernel network stack and process packets efficiently at Arista switches, a single switch CPU core can sustain 10~$\mu$s beacon interval.

As shown in Figure~\ref{fig:reorder-overhead}, although the maximal buffer size required on the receiver increases linearly with beacon interval, the message reordering throughput of a CPU core does not degrade significantly.

\parab{Background traffic.}
Figure~\ref{fig:clock-sync} compares reordering delay of minimax clock synchronization with physical clock synchronization (PTP). Without background traffic, minimax clock synchronization achieves similar accuracy with PTP. In the presence of background TCP traffic, physical clock synchronization leads to high reordering delay, while \sys adapts to variable congestion delay.


\iffalse
\subsubsection{Incremental Deployment}
\label{sec:eval-incremental}


\begin{figure}[t]
\centering
        \subfloat[Clock convergence.\label{fig:clock-convergence}]
        {\includegraphics[width=.23\textwidth]{images/fixme.pdf}}
        \hspace{0.01\textwidth}
        \subfloat[Reordering delay.\label{fig:incremental-delay}]
        {\includegraphics[width=.23\textwidth]{images/fixme.pdf}}
\caption{[Simulation] Merging two running TOMS systems by adding a network link between them.}
\label{fig:incremental}
\end{figure}

Figure~\ref{fig:clock-convergence} shows
Converge time graph, compare with physical time synchronization solution

Figure~\ref{fig:incremental-delay} shows the reordering delay. (spike then fall back)
\fi
