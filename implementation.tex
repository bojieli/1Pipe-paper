\iffalse
\begin{table*}[thbp]
	\centering
	\scalebox{0.9}{
	\begin{tabularx}{1.1\textwidth}{l|X}
		\hline
		$T$ = Scatter([($M_i$, $R_i$)]) & Scatter a group of messages $M_i$ to receivers $R_i$ respectively, return timestamp $T$. \\
		\hline
		CompletionCallback($f$($T$, IsDelivered, IsFast)) & Register callback $f$ to notify whether a sent scattering $T$ is delivered or discarded, and whether it is an one-RTT fast delivery. \\
		\hline
		$T, M$ = TentativeRecv() & Receive a message $M$ and its timestamp $T$ in order, but may be recalled. \\
		\hline
		RecallCallback($f$($T, M$)) & Register callback $f$ to recall a tentatively received message $M$ at timestamp $T$. \\
		\hline
		$T, M$ = ReliableRecv() & Receive a message $M$ and its timestamp $T$ reliably in order. \\
		\hline
		\hline
		ID = Init(IsHostOrSwitch) & Initialize \sys and get a node ID. \\
		\hline
		Connect(NeighborID) & Connect to a neighbor node (switch or host). \\
		\hline
		Disconnect(NeighborID) & Disconnect from a neighbor node (switch or host). \\
		\hline
		FailureCallback($f$(NodeID, $T$)) & Register callback $f$ for failure notification of node ID at timestamp $T$. \\
		\hline
		NewID = Recover(OldID) & Get a new ID after failure recovery. \\
		\hline
		[$T_i, M_i$] = RedoRecv($T$) & Redo reliable receive since timestamp $T$. \\
		\hline
		DiscardLog($T$) & Discard redo log until timestamp $T$. \\
		\hline
	\end{tabularx}
	}
	\caption{\sys API.}
	\vspace{-25pt}
	\label{tab:api}
\end{table*}

\begin{figure*}[t!]
	\centering
	\subfloat[\revise{Programmable} switching chips.\label{fig:p4}]
	{\includegraphics[width=.30\textwidth]{images/p4_implementation.pdf}}
	\hspace{0.04\textwidth}
	\subfloat[Switch CPU.\label{fig:commodity}]
	{\includegraphics[width=.25\textwidth]{images/commodity_implementation.pdf}}
	\hspace{0.04\textwidth}
	\subfloat[End hosts only.\label{fig:end-host}]
	{\includegraphics[width=.28\textwidth]{images/endhostonly_implementation.pdf}}
	\vspace{-5pt}
	\caption{\sys dataflow in networks with different programming capabilities.}
	\label{fig:impl}
	\vspace{-15pt}
\end{figure*}
\fi

\section{Implementation}
\label{sec:impl}

%\sys{} is implemented on both end hosts and network switches.

%At the end host, \sys requires a reliable transport. The transport can be built over any at-most-once packet transmission interfaces, \textit{e.g.}, UDP, UNIX raw socket, RDMA unreliable datagram~\cite{infinibandrocev2} or netmap~\cite{rizzo2012netmap}. We implement the message transport on the top of a user-mode TCP stack~\cite{dunkels2001design} for loss recovery and congestion control. Beacon packets are sent directly, bypassing the TCP stack. To ensure that retransmitted packets do not violate timestamp monotonicity, we add a TCP option to mark them. In addition, we maintain a mapping from TCP sequence numbers to timestamps, and update commit barrier when a TCP ACK is received.


\subsection{Processing on End Hosts}

%\RED{Why not RC? Why UD? because RC QPs are out of order. End-to-end flow control and congestion control before assigning the timestamp. Flow control: initial window, after receiving the first packet, receiver allocate fixed receive window according to estimated BDP. Congestion control: can follow DCQCN but did not implement. In RoCEv2, PFC works on Ethernet link layer, can co-work with RDMA UD. RDMA UD is back-pressured by PFC, so the new UD WQEs fail to enqueue, and then return failure to application... Deadlock: flow control vs. barrier message. Congestion control: when multicast a group of messages, wait until all destinations have flow control or congestion control window, then get timestamp and send out.}


We implement an \sys{} library, \textit{lib1pipe}, at the end host. The library is built on top of RDMA verbs API.
\sys{} obtains timestamp from CPU cycle counter and assigns it to messages in software.
Because RDMA RC buffers messages in different QPs, we cannot ensure timestamp monotonicity on the NIC-to-ToR link.
\revise{Ideally, we would like to use a SmartNIC and attach timestamps to packets when they egress to the port.
However, because we only have access to standard RDMA NICs, }we use RDMA UD instead.
Each \sys{} message is fragmented into one or more UD packets.%, where \sys{} messages larger than MTU are fragmented into multiple UD packets.

\sys{} implements end-to-end flow and congestion control in software. When a destination process is first accessed, it establishes a connection with the source process and provisions a receive buffer for it, whose size is the receive window. A packet sequence number (PSN) is used for loss detection and defragmentation. Congestion control follows DCTCP~\cite{alizadeh2010data} where ECN mark is in the UD header. When a scattering is sent by the application, it is stored in a send buffer. \finalrevise{If the send buffer} is full, the send API returns fail.
\revise{Each destination maintains a send window, which is the minimum of the receive and congestion windows.
When all messages of a scattering in send buffer are within the send window for the corresponding destination, they are attached with the current timestamp and sent out.}
This means that when some destinations or network paths of a scattering are congested, it is held back in the send buffer rather than slowing down the entire network.
\revise{To avoid live-locks, a scattering acquires ``credits'' from the send windows. If the send window for a destination is insufficient, the scattering is held in a wait queue without releasing credits. This makes sure that large scatterings can eventually be sent, at the cost of wasting credits that could be used to send other scatterings out-of-order.}
Beacon packets are sent to the ToR switch, and they are not blocked by flow control.

A UD packet in \sys{} adds 24 bytes of headers: 3 timestamps including message, best-effort barrier, and commit barrier; PSN; an opcode and a flag that marks end of message.
A timestamp is a 48-bit integer, indicating the number of nanoseconds passed on the host. %Since the timestamps wrap around in 4 seconds, 
We use PAWS~\cite{jacobson1992tcp} to handle the timestamp wrap around.
%\footnote{Barriers for best effort and reliable \sys{} are independent. If a cluster only needs one service, only one barrier is needed.}

%\sys{} uses PFC~\cite{pfc} to avoid congestion loss. We leave congestion control to future work.

%For reliable \sys{}, we use RDMA Reliable Connection (RC) to send messages in prepare phase, and use UD to send commit barrier to ToR switch.
%An RC message has 6 bytes of header, representing the timestamp.
%The UD message has the same format as best effort \sys{}.
%We maintain a receive buffer to reorder messages before delivery and a send buffer to track unACKed messages.

When \textit{lib1pipe} initializes, it registers to the controller and spawns a polling thread to: (1) generate periodic beacon packets; (2) poll RDMA completion queues and process received packets, including generating end-to-end ACKs and retransmitting lost packets; (3) reorder messages \finalrevise{in the receive buffer} and deliver them to application threads.
\textit{lib1pipe} uses polling rather than interrupt because RDMA RTT is only $1\sim2 \mu$s, while interrupt would add $\sim10 \mu$s of delay~\cite{yang2012poll}.

Processes within a host are exposed directly to the Top-of-Rack (ToR) switch(es), and the ToR aggregates timestamps of all processes in the rack.
In future work, the software process of \textit{lib1pipe} may be offloaded to a programmable NIC~\cite{kaufmann2016high,firestone2018azure}, which assigns timestamps to messages on egress pipeline.

The controller is a replicated service that stores routing graph, process information, failure notifications, and undeliverable recall messages in etcd~\cite{etcd}.
\revise{In a large network, future work can distribute the controller to a cluster, each of which serves a portion \finalrevise{of the network}.}

%\RED{finding the minimum timestamp in a single (/small) clock cycle, where you have 256 ports, or the potential need to access a register in multiple places in the pipeline (the last point must have been addressed, else the evaluation would have failed).}


%We do not need to take special care of intra-host communication between processes: because the packets loopback at the NIC, they take a shorter path than looping back at the switch, so, the packets must arrive earlier than the barrier timestamp.

\iffalse
\subsection{ROMS API on Hosts}
\label{sec:api}

An application uses ROMS via API in Table~\ref{tab:api}.
We implement a ROMS library at the end host. The library is built on top of a user-mode TCP stack~\cite{libvma,dunkels2001design} for loss recovery and congestion control. Beacon packets are sent directly, bypassing the TCP stack. To ensure that retransmitted packets do not violate timestamp monotonicity, we add a TCP option to mark them. In addition, we maintain a mapping from TCP sequence numbers to timestamps, and update delivery barrier when a TCP ACK is received.

We show an example ROMS application: externally consistent~\cite{corbett2013spanner} transactional key-value store (KVS).
The KVS is partitioned to shards according to hash of key, so clients can locate the shard for each key~\cite{nishtala2013scaling,eris}.
Each shard is replicated on multiple hosts.
An independent transaction is initiated by a client host and atomically executes independent GET or PUT operations on multiple keys, which may involve several shards and all replicas in the shards.
%With ROMS, we do not need a centralized transaction manager.
The client scatters the operations to involved shards directly using ROMS \emph{Scatter} API.
If an operation is read-only, it is scattered to one replica of the shard. Otherwise, it is scattered to all replicas of the shard.
To reduce normal case latency, each replica receives operations via \emph{TentativeRecv}, executes them and respond to clients.
However, the replica should be ready to recall tentatively received messages and the client would hold the response until \emph{CompletionCallback}.
If a scattering fails, \emph{RecallCallback} notifies all replicas.
Because removing an operation may affect the results of future operations, each replica re-executes operations since the failed scattering, and respond to clients with a redo mark.
At the same time, \emph{CompletionCallback} notifies the client. Then the client can remove the failed replica and issue a new scattering with a new timestamp.
If a scattering succeeds, the shards confirm the tentatively received messages via \emph{ReliableRecv}, log updated KVs along with timestamp and call \emph{DiscardLog}.
At the same time, \emph{CompletionCallback} notifies the client and tells whether it is an one-RTT fast delivery.
For fast delivery, the client collects responses from all involved shards and replicas.
Otherwise, \emph{i.e.}, packet loss or failure occur, the client waits for responses from all involved shards and replicas with redo mark.

To enable failure recovery, ROMS library logs received messages until \emph{DiscardLog} is called.
When a shard recovers from failure, we replay the KV log and get the last update timestamp, then redo missing operations via \emph{RedoRecv} since the timestamp.
When a client recovers from failure, the uncompleted scatterings are already discarded.
The client may simply report failures to user or implement logging to retry failed scatterings.
\fi


\subsection{In-Network Processing}
\label{sec:in-network-processing}

We implement in-network processing at three types \finalrevise{of the network} switches with different programming capabilities.


\subsubsection{\revise{Programmable} Switching Chip}
\label{sec:p4}
%A reconfigurable switching chip can maintain a handful of states $\mathcal{S}$ and process each packet $P$ through the state machine $P', \mathcal{S}' = f(P, \mathcal{S})$. Hence, we can implement the in-network processing on the reconfigurable chip.

We implement the in network processing using P4~\cite{bosshart2014p4} and compile it to \revise{Tofino~\cite{tofino}}. Because a Tofino switch has 4 pipelines, it is regarded as 4 ingress and 4 egress switches connected via 16 all-to-all links. Each of the 8 ``switches'' derives barriers independently.
\sys{} needs 2 state registers \emph{per input link}, storing the two barriers for best effort and reliable \sys{}, respectively.
%\sys{} also needs 2 state registers per switch to store the minimum of per-link barriers.
%As Figure~\ref{fig:p4} shows, 
For each packet, barrier register of the input link is updated in the first stateful pipeline stage of the switch.
Because each stage can only compute the minimum of two barriers, the switch uses a binary tree of registers with $O(\log N)$ pipeline stages to compute the minimum link barrier $B_{new}$, where $N$ is the number of \revise{ports}.
\revise{For a typical 32 port switch, it would cost 5 stages out of the 16 ingress stages.}
At the final pipeline stage, the barrier field in packet is updated to $B_{new}$.
The control plane software routinely checks link barriers and reports failure if a link barrier significantly lags behind.
\revise{The expected delay of best effort \sys{} is (base delay + clock skew) when links are fully utilized, or (base delay + beacon interval/2 + clock skew) when most links are idle.}


%We implement the in-network processing using P4~\cite{bosshart2014p4} and compile it to Barefoot Tofino~\cite{tofino}. \sys needs 8 state registers per input link and 5 state registers per output link. Each data packet carries three timestamp fields: message timestamp, loss-free barrier and delivery barrier, as well as loss encountered flag (Algorithm~\ref{alg:loss-detection}).
%A timestamp is a 6-byte integer, indicating the number of nanoseconds passed on the host. %Since the timestamps wrap around in 7.8 hours, 
%We use PAWS~\cite{jacobson1992tcp} to handle the timestamp wrap around.
%%to compare timestamps: If $s$ and $t$ are timestamps, $s < t$ if and only if $0 < t - s < 2^{47}$, computed in 48-bit unsigned arithmetic. 
%As shown in Figure~\ref{fig:p4}, the beacon packets carries not only the same set of timestamps and flag as data packets, but also information for minimax clock synchronization.
%% (Algorithm~\ref{alg:minimax}).

\subsubsection{Switch CPU}
\label{sec:commodity}

For a switch without a \revise{programmable} switching chip, \textit{e.g.,} Arista 7060 which uses Broadcom Tomahawk chip, we implement in-network processing on the switch CPU.
%Data-plane programmable switches are currently not widely available in data centers.
Although commodity switches cannot process packets in data plane, they have a CPU to process control-plane packets, analogous to directly connecting a server to a port of the switch.
Compared to server CPUs and NICs, the switch CPU is typically less powerful (\textit{e.g.}, 4 cores at 1~GHz) and has lower bandwidth (\textit{e.g.}, 1 Gbps).
Because the switch CPU cannot process every packet, data packets are forwarded by the switching chip directly. %, as Figure~\ref{fig:commodity} shows.
The CPU sends beacons periodically on each output link, regardless of whether the link is idle or busy.
Received barriers in beacons are stored in registers per input link.
A thread on the CPU periodically computes the minimum of link barriers and broadcasts new beacons to all output links.
Computing the minimum barrier takes hundreds of cycles, which is not a bottleneck compared to the cost of broadcast.
Because data and beacon packets are FIFO in switch queues and on network links, the barrier property is preserved. On receivers, buffered data packets are delivered to the application according to barriers in beacon packets.
Compared with the \revise{programmable} chip, switch CPU \revise{has higher latency due to CPU processing.}
\revise{So, the expected delay is base delay + (switch CPU processing delay $\times$ number of hops + beacon interval/2 + clock skew).}

\begin{figure*}[t]
	\begin{minipage}[]{.32\textwidth}
		\centering
		\subfloat[Throughput.\label{fig:total-order-tput}]{
			\includegraphics[width=\textwidth]{gnuplot/total_order.eps}
		}
		\newline
		\vspace{-5pt}
		\subfloat[Latency.\label{fig:total-order-lat}]{
			\includegraphics[width=\textwidth]{gnuplot/total_order_lat.eps}
		}
		\caption{Scalability comparison of total order broadcast algorithms.}
		\label{fig:total-order}
	\end{minipage}
	\hspace{0.01\textwidth}
	\begin{minipage}[]{.32\textwidth}
		\centering
		\subfloat[Scalability on testbed. Error bars show $5^{th}$ and $95^{th}$ percentile.\label{fig:reorder-testbed}]{
			\includegraphics[width=\textwidth]{gnuplot/reorder_testbed.eps}
		}
		\newline
		\vspace{-15pt}
		\subfloat[Simulation of varying packet loss rates.\label{fig:reorder-loss}]{
			\includegraphics[width=\textwidth]{gnuplot/loss_latency.eps}
		}
		%\subfloat[Simulation with different end-host delays.
		%T-R\textit{i} shows minimax clock synchronization, P-R\textit{i} shows physical clock synchronization.\label{fig:reorder-simulation}]
		%{\includegraphics[width=\textwidth]{gnuplot/reorder_simulation.eps}}
		\caption{Message delivery latency of \sys{} variants.}
	\end{minipage}
	\hspace{0.01\textwidth}
	\begin{minipage}[]{.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{gnuplot/failure_recovery.eps}
		\caption{Failure recovery time of reliable \sys{}. Error bars show $5^{th}$ and $95^{th}$ percentile.}
		\label{fig:failure-recovery}
		\includegraphics[width=\textwidth]{gnuplot/reorder_receiver.eps}
		\caption{Reorder overhead on a host.}
		\label{fig:reorder-overhead}
	\end{minipage}
	%\vspace{-1.6em}
	\vspace{-10pt}
\end{figure*}


\subsubsection{Delegate Switch Processing to a Host}
\label{sec:end-host}

If the switch vendor does not expose access interfaces to switch CPUs, we can offload the beacon processing to end hosts. \revise{We designate an \emph{end-host representative} for each network switch. The challenge is that best effort \sys{} requires the barrier timestamp to be the lower bound of future message timestamps on a network link $L$. So, beacons with barrier timestamps on $L$ must pass through $L$.}
%As Figure~\ref{fig:end-host} shows, 
That is, for two directly connected switches $S_1, S_2$ and their representatives $H_1, H_2$, beacon packets from $H_1$ to $H_2$ need to go through the link $S_1 \rightarrow S_2$. If the routing path between two representatives does not go through $S_1 \rightarrow S_2$, beacon packets needs to detour: they are sent with three layers of IP headers: $H_1 \rightarrow S_1$, $S_1 \rightarrow S_2$, and $S_2 \rightarrow H_2$.
We install tunnel termination rules in each network switch to de-capsulate one layer of IP header, so the beacon packet will traverse through $H_1 \rightarrow S_1 \rightarrow S_2 \rightarrow H_2$.

Beacon packets use one-sided RDMA write to update barriers on representative host.
Similar to Sec.\ref{sec:commodity}, minimum barriers are periodically computed and broadcast to downstream representatives.
\revise{The expected delay is base delay + ((RTT between switch and host + host processing delay) $\times$ number of hops + beacon interval/2 + clock skew).}
\revise{Because CPUs on end hosts may have shorter processing delay (via RDMA) than switch CPU (via OS IP stack), host delegation may have shorter overall delay. This is why we use host delegation for evaluations in Sec.\ref{sec:evaluation}.}
