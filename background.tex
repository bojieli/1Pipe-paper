\section{Goals and Background}
\label{sec:background}

\iffalse
\textcolor{red}{Outline:\\
1. Design Goals (remove corretness)
2. Data Center Netowkrs: loop-free
3. Programmable switches in Data Center Netowkrs: FIFO
}
\fi

\subsection{Design Goals}
\label{sec:goals}

Our goal is to build a scalable total-order message scattering service on the top of data center networks. Such a service should satisfy the following requirements.

%\textbf{Correctness.}
%A sender scatters a group of messages with a same timestamp to multiple receivers, called a \textit{scattering}.
%Timestamps for different scatterings are unique and increasing on each sender.
%Each receiver delivers messages to applications in timestamp order (break ties by sender ID).
%Messages in each scattering are delivered by either all or none receivers.

\parab{Efficient.}
First, the \textit{reordering delay} between end host receiving the message and delivering the message to application should be minimal.
This requires senders to assign timestamps properly so that messages with similar timestamps arrive at receivers as synchronized as possible.
Second, the network and computation overhead of total ordering should be minimal.

\parab{Scalable.}
The system should be scalable with number of switches and hosts, which implies no centralized coordinator and the overhead should not increase too much with system scale.
Additionally, for ease of deployment, it is also desirable for the algorithms running on each host and switch to be logically equivalent.



%\textbf{Low Network Bandwidth Overhead.}
%Additional packets generated for ensuring ordering should be minimal. The metadata tagged on each packet should also be minimal.

%\textbf{Low CPU Overhead.}
%Because the CPUs on switches are typically not very powerful, the processing on switch CPU should be as simple and infrequent as possible. Moreover, the reordering overhead on end hosts should also be reasonable.

\parab{Fault Tolerant.}
The correctness of the system should not be violated under packet loss, or failure of switches, links or end hosts.
The system ensures liveness unless network partition occurs.


\parab{Readily Deployable.}
The service should be readily deployable using existing commodity switching hardware in data centers.

%\textbf{Adaptive to Delay Variance.}
%The delay among geographically distributed datacenters can be hundreds of milliseconds.
%Because commodity NIC hardware is not able to assign a same timestamp to a group of scattered messages, \sys assigns timestamps in software.
%Different OS network stacks have more than 100~$\mu$s of processing delay differences, and it may grow to milliseconds under heavy load.
%Links with different speeds or congestions also lead to delay variations.
%The system should dynamically adapt to the variance of network delays to minimize reordering delay.

\subsection{Data Center Network}
\label{sec:dcn}


%\begin{figure*}[t]
%	\centering
%	\subfloat[Physical topology.\label{fig:dcn_tpo}]
%	{\includegraphics[width=.45\textwidth}{images/dcn_topology.pdf}}
%	\hspace{0.09\textwidth}
%	\subfloat[Expanded topology.\label{fig:dcn_dag}]
%	{\includegraphics[width=.45\textwidth]{images/dcn_dag.pdf}}
%	\caption{Architecture of a typical data center network.}
%	\label{fig:dcn}
%\end{figure*}


\begin{figure}[t]
\centering
{\includegraphics[width=.48\textwidth]{images/dcn_dag.pdf}}
\caption{
	Routing topology of a typical data center network.
	Each physical switch is split into two logical switches, one for \textit{uplink} and one for \textit{downlink}.
    The dashed virtual link between corresponding uplink and downlink switch indicates ``hairpin'' traffic from a lower-layer switch or host to another one.
}
\label{fig:dcn}
\vspace{-1.5em}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.3\textwidth]{images/cropped_switch_architecture.pdf}
\caption{Architecture of a typical network switch.}
\label{fig:switch}
\vspace{-1.5em}
\end{figure}

Modern data centers typically adopt multi-rooted tree topologies~\cite{leiserson1985fat,greenberg2009vl2} to interconnect hundreds of thousands of end hosts.
Shortest-path routing between two hosts in a multi-rooted tree first goes up layers of switches to one of their common ancestors, then goes down layers of switches.
Therefore, the routing topology form a directed acyclic graph (DAG), as shown in Figure~\ref{fig:dcn}.
This loop-free topology enables hierarchical aggregation.

%\RED{Discuss loop-free routing.}
%Formally, for $n$ distinct switches $S_1, \ldots, S_n$, if there are $n$ links $L_1, \ldots, L_n$ directly connecting $S_1 \rightarrow S_2, \ldots, S_{n-1} \rightarrow S_n, S_n \rightarrow S_1$, and $n$ possible routes (need not be distinct) passing through $L_1 \rightarrow L_2, \ldots, L_{n-1} \rightarrow L_n, L_n \rightarrow L_1$ respectively, then we call it a \textit{routing loop}.
%Loop-free is a liveness requirement.
%A routing loop stalls the update of timestamp barriers, but does not violate correctness.

\subsection{Programmable Switches}
\label{sec:programmable-switches}


A typical data center switch consists of a switching chip~\cite{broadcom} and a CPU (Figure~\ref{fig:switch}).
The switch operating system~\cite{arista-eos} software running on the CPU controls the switching chip (\textit{e.g.} configure chip register values).
The switching chip can forward selected traffic (typically control plane packets, such as DHCP and BGP) to CPU via a virtual NIC.

The switching chip is composed of an \textit{ingress} pipeline, multiple FIFO queues and an \textit{egress} pipeline.
When a packet is received from an ingress link, it first goes through the ingress pipeline to determine the egress link and queueing priority, then get buffered into the corresponding FIFO queue.
The egress pipeline pulls packets from queues according to priority, applies header modifications and sends them to egress links.
This queueing model ensures FIFO property for packets with a same priority.
In addition, packet corruption on ingress links and buffer overflow inside switches can be detected, and a packet loss counter is incremented per lost packet.

The data center switch can provide good programmability.
First, the CPU can be used to co-process (a small amount of) data plane packets with high flexibility~\cite{lu2011serverswitch}.
Second, the switching chip is becoming more and more programmable in recent years.
For example, Tofino chip from Barefoot networks~\cite{tofino} 
%uses RMT architecture~\cite{bosshart2013forwarding} with 
has flexible packet parsers and reconfigurable match action table pipelines that can be programmed using P4~\cite{bosshart2014p4} to achieve \textit{flexible per-packet processing}.

Despite good programmability, the data center switch typically has limited buffer resource.
The average per-port on-chip buffer is typically hundreds of kilobytes~\cite{bai2017congestion} in size.
Furthermore, the buffer size per port per Gbps keeps decreasing with the increase of link speed.
As a result, it is challenging to buffer many packets at the switches in a data center.


%, controls the switching chip and communicates with other switches via a management network. 

%There is a CPU port on the switching chip and a virtual NIC on the CPU to transfer control-plane packets (\textit{e.g.} DHCP, BGP) between the chip and the CPU. 


%Logically, the switching chip is composed of an \textit{ingress} pipeline, multiple queues and an \textit{egress} pipeline.
%Both ingress and egress pipelines are a chain of match-action tables. Each table can accomodate a number of rules. Each rule matches a set of packet header fields and internal states, and applies a limited repertoire of actions corresponding to common processing behaviors.
%When a packet is received from an ingress link, it first goes through the ingress pipeline (including tunnel decapsulation, routing, ACL rules and others) to determine the egress port and queueing priority, then get buffered into the tail of the queue of corresponding priority. The egress pipeline pulls packets from the head of each queue according to scheduling disciplines, applies header modifications and sends them to the egress links. This queueing model ensures FIFO property for packets with same priority.

%One limitation of most widely-deployed switching chips is the limited programmability of internal states that are persistent across packets. The Barefoot Tofino switching chip~\cite{tofino} using RMT architecture~\cite{bosshart2013forwarding} supports \textit{stateful per-packet processing} and has high programmability with P4~\cite{bosshart2014p4} language.
%However, the number of persistent states supported by each switch is only sufficient to have several per-port registers, but not enough to maintain per-packet states. 
%Furthermore, the buffer size of network switches is less than 1~MB per port~\cite{bai2017congestion}.
%Therefore, it is hard to implement a strict priority queue large enough for in-network serialization~\cite{sivaraman2016programmable}.

%\subsection{In-Network Computation}
%\label{sec:in-network}

%In-network computation flourishes with the trend of programmable switches (cite P4, Domino, serverswitch) and NICs (cite ClickNP, flexnic, VFP), opening up two categories of opportunities to distributed systems:

%\textit{Increase knowledge and control in network transmission.}
%In modern datacenters, there are multiple paths from a sender to a receiver for load balancing and fault tolerance. 
%Software-defined networking (SDN) enables visibility to routing table changes from BGP or administrators. In our scenario, in order to know the set of all possible ingress ports per egress port, \sys software on the affected switches need to be notified prior to route change.

%\textit{Offload computation on end hosts to reduce network traffic and latency.}
%\textit{Multicast} is an in-network computation to reduce duplication of messages on sender side.
%\textit{On-switch cache} is also a type of in-network computation to save computation on end hosts and improve load balancing (cite switchKV).
%A single network switch is a serialization point where \textit{coordination} of directly-connected hosts can take place (cite hardware coordination, Eris).
%\sys leverages the broadcast, cache and coordination functionalities of network switches to scale total ordering hierarchically.\RED{(leverage broadcast and cache?)}



\iffalse
\subsection{Network Requirements}
\label{sec:assumptions}

\sys assumes that the underlying network satisfies three properties, two for correctness and one for liveness.

\textbf{At-most-once transport.}
The underlying transport of \sys should provide at-most-once semantics, \textit{i.e.}, each packet is transmitted at most once.
Otherwise, the retransmitted packet may violate timestamp ordering on the network link.
UDP/IP or unreliable datagram in RDMA satisfies this requirement.

\textbf{FIFO on a Network Switch.}
If packets $P_1, P_2$ arrive in order to an ingress port of switch $S$, and they are routed to the same egress port, then their egress ordering should also be $P_1, P_2$.
In practice, packets from the same application typically have the same priority, so packet ordering is preserved after passing through a single switch.

\textbf{Loop-Free Routing.}
%Formally, for a switch $S$, its egress ports $E$ and ingress ports $I$, if there is a possible route from an ingress port $i \in I$ to an egress port $e \in E$, then add $i$ to the \textit{ingress set} of $e$.
Formally, for $n$ distinct switches $S_1, \ldots, S_n$, if there are $n$ links $L_1, \ldots, L_n$ directly connecting $S_1 \rightarrow S_2, \ldots, S_{n-1} \rightarrow S_n, S_n \rightarrow S_1$, and $n$ possible routes (need not be distinct) passing through $L_1 \rightarrow L_2, \ldots, L_{n-1} \rightarrow L_n, L_n \rightarrow L_1$ respectively, then we call it a \textit{routing loop}.
Loop-free is a liveness requirement.
A routing loop stalls the update of timestamp barriers, but does not violate correctness.
%Sec.~\ref{sec:eval-changes} evaluates temporary routing loops.
%Shortest-path (ECMP) routing in a Clos network is loop-free, which is common in datacenters.
%Shortest-path routing in a triangle is also loop-free. However, shortest-path routing in a pentagon is not loop-free, because the 5 routes $S_1 \rightarrow S_2 \rightarrow S_3, \ldots, S_5 \rightarrow S_1 \rightarrow S_2$ form a routing loop by our definition.
\fi