\subsection*{Abstract}

The ability to have reliable and ordered delivery of a group of messages can facilitate and simplify many distributed applications.
Existing approaches either employ centralized sequencers or tokens, thus suffer from limited scalability, or use distributed consensus protocols, which incurs high overhead in bandwidth and delay.

This paper proposes Reliable Ordered Message Scattering (\sys), an efficient and scalable method to scatter groups of messages in linearizable and serializable order via data center network.
\sys is linearizable as the message delivery order of each host is identical to the wall clock order when the messages are sent.
\sys is seralizable as messages in a scattering group are delivered if and only if none of its participants fail.
\sys guarantees correctness and liveness under packet loss, Byzantine failure of hosts and crash failure of network switches.
\sys is scalable as it distributes work to each switch and end host.
At its core, \sys achieves synchrony by separating the bookkeeping of order information from message forwarding.
Network switches aggregate order information and detect packet loss, while end hosts reorder and retransmit messages.
\sys achieves consensus by leverage switches to detect and notify host failures.

%\sys is scalable
%and fault tolerant 
%as it distributes work to each switch and end host. It is reliable in the sense that a message is guaranteed to be delivered exactly once to a non-faulty host.
%At its core, \sys separates the bookkeeping of order information from message forwarding.
%\sys aggregates order information using in-network computation at switches.
%This forms the ``control plane'' of the system.
%On the ``data plane'', \sys forwards messages in the network as usual and reorders them at the receiver-end based on the order information.
%In order to reduce CPU and network overhead, and minimize message reordering latency, \sys employs multiple optimization techniques such as hierarchical barrier merging, idle network beaconing, minimax clock synchronization and in-network loss detection.

%The ability to have total ordering of events and messages can simplify and accelerate many distributed applications. Traditional approaches to achieve total ordering either employs centralized sequencers or tokens with limited scalability, or use fully distributed consensus protocols with bandwidth and latency overheads.

%In this paper, we propose Total-Order Message Scattering (TOMS), an efficient and scalable solution to address this problem. Driven by recent progress in programmable networks, we leverage in-network computation at the switches to provide order information, while use end hosts to buffer and reorder messages and achieve in-order message delivery. Due to this clear separation of control and data plane, \sys is scalable as it distributes the work to distributed end hosts and switches. In addition, we also make various efforts, such as hierarchical barrier merging and minimax clock synchronization, to ensure \sys's efficiency. \textcolor{red}{I am not sure whether the last sentence is correct}  


%Totally ordered events and messages can simplify and accelerate many distributed applications. To achieve this, existing approaches either leverages centralized sequencers or tokens, suffering from limited scalability, or use distributed consensus protocols, which is inefficient due to bandwidth and latency overheads. 

%In this paper, we propose Total-Order Message Scattering (TOMS), an efficient and scalable solution to address this problem. Driven by recent progress in programmable networks, we leverage in-network computation at the switch to provide necessary order information. With the order information, the receiver can efficiently reorder and process buffered messages. \sys is scalable as it distributes the work to distributed end hosts and switches. In addition, we also make various efforts, such as hierarchical barrier merging and minimax clock synchronization, to ensure \sys's efficiency. \textcolor{red}{I am not sure whether the last sentence is correct}  

%to efficiently derive timestamp barriers, we propose hierarchical barrier merging, and sending beacons on idle links. To minimize reordering delay, we propose a minimax clock synchronization scheme to assign timestamps.   

%The ability to have total ordering of events and messages can simplify and accelerate many distributed applications. Traditional approaches to achieve total ordering either employs centralized sequencers or tokens with limited scalability, or use fully distributed consensus protocols with bandwidth and latency overheads. Recent availabilities of programmable datacenter networks give rise to proposals to co-designing network and distributed systems together. This paper follows this trend and proposes an efficient and scalable Total-Order Message Scattering (TOMS) solution using in-network computation.

%Due to limited buffering in network switches, TOMS separates the control plane that keeps ordering information from the data forwarding plane. Messages are timestamped on the senders and buffered and reordered in end-host receivers. The control plane provides a barrier (lower bound) of future timestamps to each receiver.
%To efficiently derive timestamp barriers, we propose hierarchical barrier merging, and sending beacons on idle links.
%To minimize reordering delay, we propose a minimax clock synchronization scheme to assign timestamps.

%TOMS achieves high performance, low network and CPU overhead, scalability, fault tolerance and incremental deploy-ability using commodity hardware in datacenter networks.
We build two \sys prototypes using Barefoot and Arista switches.
Our evaluation shows that \sys achieves high performance and fault tolerance with low CPU and network overheads.
As a case study, \sys achieves externally consistent independent transactions with performance close to a non-transactional system and scales linearly with the number of hosts.
Compared to MVCC concurrency control and Paxos-based replication, \sys improves throughput by 50x under YCSB+T workload and 100x for TPC-C Payment transactions.
%As case studies, \sys improves atomic multi-site operation throughput by 50x under YCSB+T workload, achieves 100x scalability for TPC-C Payment transactions and scales serializable log replication.
