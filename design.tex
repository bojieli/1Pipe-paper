\section{Design}
\label{sec:design}

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{images/derive_barriers.pdf}
\caption{Deriving barriers with senders $R, W$ and receivers Shard($a$), Shard($b$). Assume $T_W < T_R$.}
\label{fig:barrier}
\vspace{-1em}
\end{figure}

In Figure~\ref{fig:barrier}, let's reconsider the example in Sec.\ref{sec:toms}.
When shard($a$) receives both messages $R_a$ and $W_a$, due to monotonic timestamp assignment on both sender and the FIFO property between pairs of sender and receiver, shard($a$) knows that it will no longer receive timestamp $T \leq \min(T_R, T_W)$ from both $R$ and $W$.
If there are no other senders, shard($a$) can safely process messages with $T \leq \min(T_R, T_W)$.%\RED{\sout{, \textit{i.e.}, the lower timestamp one between messages $R_a$ and $W_a$}}.
%$min(T_R, T_W)$ is the lower bound of timestamps $A$ may ever receive in the future, we call it a \textit{barrier}.

Several challenges arise:

\textbf{Liveness}.
In above example, if one of $R$ and $W$ stops sending messages, shard($a$) can not deliver messages with timestamps higher than $\min(T_R,T_W)$, causing a livelock.
To ensure liveness, each sender needs to send \textit{beacon} messages to each receiver  periodically when it has no data to send.
This is covered in Sec.\ref{sec:beacon}.

\textbf{Scalability}.
If there are $N$ senders and $N$ receivers, it requires $N^2$ messages per beacon interval.
Sec.\ref{sec:ideal} leverages network switches to merge beacons hierarchically and reduce the beacon message complexity to $O(N)$.

\textbf{Reordering delay}. To minimize reordering delay, which is the delay between receiving and delivering the message, we need to synchronize the clocks at senders, as in Sec.\ref{sec:sync}.

\textbf{Packet loss}.
If a packet is lost, the receivers will observe different orderings of events. Simply retransmit the packet with same timestamp violates the monotonic property of timestamps. The detection and recovery of packet losses are discussed in Sec.\ref{sec:lossy}.

\textbf{Fault tolerance and incremental deployment}.
Sec.\ref{sec:incremental} discusses how to add end hosts, switches and links on the fly, with minimal impact on system performance, and how to merge two existing \sys systems.

%\textbf{Inter-DC topology}.
%Sec.\ref{sec:inter-dc} extends the design to inter-DC topology where routing paths are not loop-free.


\subsection{Hierarchical Merge of Barriers}
\label{sec:ideal}

A classical method of merging multiple ordered streams of packets into one ordered stream of packets is \textit{deterministic merge}~\cite{hadzilacos1994modular, aguilera2000efficient}. It works like a merge sort: Each step the network switch compares the heads of input streams and output the head packet with minimum timestamp. %Reordering packets on the switch introduces less reordering delay compared to reordering on the receivers (as shown in Figure~\ref{fig:barrier}), because the switch sees messages more frequently~\cite{aguilera2000efficient}.
Unfortunately, when any input stream is empty, the switch needs to wait and buffer the other streams in memory.
%The maximum time of waiting equals the network delay variance plus the clock skew among senders. According to Sec.\ref{sec:goals}, the delay variance may be milliseconds, therefore for a 40~Gbps network, the switch needs a 5~MB buffer per port to hold out-of-order packets. 
The buffer size in commodity switches is one order of magnitude lower than the requirement~\cite{bai2017congestion}.
Furthermore, the deterministic merge scheduling policy is not supported in commodity switches~\cite{sivaraman2015towards,sivaraman2016programmable}.


\begin{figure}[t]
\centering
\includegraphics[width=0.45\textwidth]{images/hierarchical_merge.pdf}
\caption{Hierarchical aggregation of timestamp barriers.}
\label{fig:hierarchical_merge}
\vspace{-0.9em}
\end{figure}

Our solution is to \textit{separate the control plane from the data plane}. In this case, the control plane is the ordering information, and the data plane is the messages. Instead of buffering messages at each network switch, we add a \textit{barrier timestamp} field to each packet. Thus, each packet contains two timestamp fields: a \textit{message timestamp} used by end hosts, and a \textit{barrier timestamp} used by both end hosts and switches. At begining, a packet is formed by an end host which fills the same timestamp value in both fields. As the packet moves along the route,  its barrier timestamp is modified by the switches in each link while its message timestamp is unchanged. The meaning of a barrier timestamp is as follows: 

\emph{When a switch or an end host receives a packet with barrier timestamp $B$ from a network link $L$, it indicates that all future packets from link $L$ will have higher message or barrier timestamps than $B$.}

%\RED{The barrier timestamp should have the following properties:}
%\begin{enumerate}
%\item The barrier timestamp $B$ on a network link $L$ indicates the \textit{lower bound of future message timestamps} %that travel through $L$ after $B$.
%\item \RED{(This point is included by the above property, barrier is lower bound of future data message timestamps \textbf{and} future barriers)}An additional property of the barrier timestamps is that it is \textit{non-decreasing on each network link}, so streams of barrier timestamps can be merged hierarchically.
%\end{enumerate}

%A packet on link $L$ sent earlier than barrier $B$ may still have message timestamp $M > B$. %It should be buffered in the switch in the deterministic merge approach, but in our approach 
%it is forwarded as normal and arrive at the reordering buffer on the receiver end host.

To derive barriers, a network switch maintains a register $R_i$ per ingress port $i \in \mathcal{I}$, where $\mathcal{I}$ is the set of all ingress ports. After routing a packet with barrier timestamp $B$ from ingress port $i$ to the egress port $E$, the switch updates register $R_i := B$, and modifies the barrier timestamp of the packet to a new value $B_{new}$:

%\RED{$i$ never appeared before, while $I$ did. $\exists$ is redundant}
\begin{equation}\label{equ:derive_barriers}
%\RED{
B_{new}:=\min\{R_i| i\in \mathcal{I}_E\}
%}
\end{equation}
%B := \min \{ R_i | \exists i \rightarrow E, i \in \mathcal{I} \}
Where 
\begin{equation}\label{equ:derive_barriers1}
\mathcal{I}_E =\{i| i\rightarrow E, i \in \mathcal{I} \}
\end{equation}

Here $\rightarrow$ means \textit{can be routed to}. So $\mathcal{I}_E$ is a subset of $\mathcal{I}$ that contains all ingress ports with a route to egress port $E$. $\mathcal{I}_E$ can be predetermined at route configuration time.
%Note that the switches do not read or modify message timestamps. 
In Appendix~\ref{appx:hierarchical_merge}, we prove that the hierarchical merge described in (\ref{equ:derive_barriers}) and (\ref{equ:derive_barriers1}) maintains the property of barrier timestamps.
%Appendix~\ref{appx:hierarchical_merge} proves \RED{(not several, only one)}several properties of barrier timestamps. \RED{(The only property is: any barrier timestamp or message timestamp is greater or equal to previous barrier timestamp.)}In particular, a property is that for a given packet\RED{(a given data? beacon? packet)}, its barrier timestamp is \textit{non-decreasing on each network link}.

When a receiver receives a packet with message timestamp $M$ and barrier timestamp $B$, it puts the packet into a priority queue ordered by message timestamps, then delivers all packets with message timestamp below $B$ to the application.
There may be some delay between receiving a message and receiving the corresponding barrier timestamp that allows its delivery. This delay is called \textit{reordering delay}. %\RED{\sout{Our goal is to minimize reordering delay.
%Appendix B\RED{(not proved)} proves that the reordering delay of our approach is identical to deterministic merge.}}

The liveness of the system is guaranteed if there are no \textit{routing loops} in the network (Sec.\ref{sec:goals}). If there is a routing loop of links $L_1 \rightarrow \ldots \rightarrow L_n$, then the barrier timestamps $B_{L_2} \leq B_{L_1}, \ldots, B_{L_n} \leq B_{L_{n-1}}, B_{L_1} \leq B_{L_n}$, so the barrier timestamps cannot increase. This result is evident because a packet may be forwarded in this routing loop indefinitely, therefore we cannot increase the lower bound of in-flight message timestamps. When the routing loop is removed, the barrier timestamps will start increasing again.

Although the design above works on a P4 programmable switch, many commodity switching chips are not capable of storing states $R_i$ and calculating the minimum among $R_i$.
To address this, we apply the principle of control and data plane separation again, and offload the control plane entirely to the CPU on switches.
Because the CPU on a switch has far less processing capacity than switching chips, we use beacon packets as \textit{slacked barriers} of data packets. This will be discussed in Sec.~\ref{sec:commodity}.

\subsection{Beacons on Idle Links}
\label{sec:beacon}

The liveness of the system requires knowledge from every network link. If a link is idle, the receiver cannot distinguish if it is really idle or there is excessive delay on the link, or worse, the sender or link has a failure. There are two ways to resolve the idle link problem: (1) let the sender send a signal packet to shutdown the link temporarily; (2) ensure the link is always busy by sending periodic \textit{beacons}.

The first solution requires the sender to rejoin the receiver when it has another packet to send, which requires at least a single-hop RTT to re-sync the timestamps (Sec.\ref{sec:incremental}). Therefore, this solution is applicable when the sender knows it would not send any packet during a relatively long period, \textit{e.g.}, when the application closes.

The second solution adds network and CPU overhead for the beacons. The beacon interval $\mathcal{B}$ affects the reordering delay of the whole system. The receiver side of an idle link can only increase its barrier timestamp every time it receives a beacon, so the maximum reordering delay added by beacons is $\mathcal{B} \cdot h$, where $h$ is the number of network hops. The network/CPU overhead and reordering delay is a trade-off to choose a proper beacon interval.

To minimize reordering delay, we hope to synchronize the beacons at different network links, so the reordering delay can be reduced to $\mathcal{B}$ regardless of network hops. This requires a network switch to send a beacon as soon as it receives a beacon from ingress links. So we introduce \textit{min beacon interval} and \textit{max beacon interval}, in Algorithm~\ref{alg:beacon}.

\setlength{\textfloatsep}{1em}
\begin{algorithm}[t]
 \DontPrintSemicolon
 \textbf{Parameters:} beacon intervals $min$, $max$\;
 \textbf{States:} Ingress port barrier timestamp $R_i, i \in \mathcal{I}$\;
   \qquad Egress port barrier timestamp $B_e, e \in \mathcal{E}$\;
   \qquad Last sent barrier time last$_e, e \in \mathcal{E}$\;
   \qquad Local physical clock, fetched via \textit{time()}\;
   \qquad timer$_e, e \in \mathcal{E}$ that fires after a given interval\;
 \SetKwProg{Fn}{Function}{ begin}{end}
 \SetKwFunction{SendBeacon}{SendBeacon}
 \Fn{\SendBeacon{\textnormal{barrier} $B_E$, \textnormal{egress port} $E$}}{
    Send beacon packet $B_E$ to $E$\;
    last$_E \leftarrow$ time()\;
    reset timer$_E$ to fire after $max$\;
 }
 \SetKwFunction{UpdateEgressPortTS}{UpdateEgressPortTS}
 \Fn{\UpdateEgressPortTS{\textnormal{egress port} $E$}}{
    $B_E \leftarrow \min \{ R_i | i \rightarrow E, i \in \mathcal{I} \}$\;
    \eIf{last$_E + min \leq$ time()}{
      SendBeacon ($B_E$, $E$)\;
    }{
      reset timer$_E$ to fire after last$_E + min - $time()\;
    }
 }
 \textbf{Event processing:}\\
 \If{receive a beacon or data packet $P$}{
  read barrier timestamp $b$ from packet $P$\;
  $R_I \leftarrow b$, where $I$ is the ingress port of $P$\;
  \If{$P$ is a data packet}{
  	determine egress port $E$ of $P$ by routing\;
    UpdateEgressPortTS ($E$)\;
    modify barrier timestamp in $P$ to $B_E$\;
  }
  \ElseIf{$P$ is a beacon packet}{
  	\ForEach{$e \in \{ e \in \mathcal{E} | I \rightarrow e \}$}{
    	UpdateEgressPortTS ($e$)\;
    }
  }
 }
 \ElseIf{timer$_E, E \in \mathcal{E}$ fires}{
    SendBeacon ($B_E$, $E$)\;
 }
 \caption{Timestamp processing with beacons.}
 \label{alg:beacon}
\end{algorithm}

%In order to detect failures, each switch has a timeout timer per ingress port. If no beacon or data packet is received for three max beacon intervals, the ingress port is considered to be dead and removed from the ingress port list. When the failed link, host or switch recovers, it needs to rejoin (Sec.\ref{sec:incremental}).

\subsection{Minimax Clock Synchronization}
\label{sec:sync}

%\RED{\sout{People perceive two events as simultaneous when observing their effects at the same time.}}
%Due to different propagation delays from the origin of events to the observers, simultaneity of events is relative to observers.
From an end-host receiver's perspective, two events should be simultaneous if the messages originated from the events reach the receiver simultaneously.
%In a distributed system, events are timestamped and their effects propagate in the form of packets.
%On one hand, we hope to timestamp the events so that if two packets originated from two distant events arrive at a receiver simultaneously, these two events should have a same timestamp.
%On the other hand, unlike our physical universe, a distributed system is easier to reason if the ordering of events is globally consistent, \textit{i.e.}, all receivers agree on the timestamps of events.
If we simply synchronize the clocks using physical clock synchronization, \textit{e.g.}, NTP~\cite{mills1991internet}, PTP~\cite{correll2005design} or DTP~\cite{lee2016globally}, the packets received from a low latency path always need to wait for the packets received from a higher latency path. This increase the reordering delay to at least the maximum delay variance of all senders.
Different OS network stacks have more than 100~$\mu$s of processing delay differences, and it may grow to milliseconds under heavy load.
Links with different speeds or congestions also lead to delay variations.

We assign an \textit{event timestamp} to each event and its emitted messages, such that:
\begin{enumerate}
\item Event timestamps strictly increase monotonically on each host.
\item Event timestamps increase at the pace of a physical clock on average.
\item Event timestamps should be assigned so that messages from different end hosts with near timestamps arrive at receivers at approximately same receiver local physical time.
\item Event timestamps should converge in a network with stable link delays and clock speed, given any initial timestamp.
\end{enumerate}

The first property is necessary for correctness. The second property ensures numerical stability, \textit{i.e.}, the increase speed of timestamps does not vanish or explode. In addition, the second property enables us to obtain an unbiased estimate of the event timestamp after one round trip, by measuring RTT with a local physical clock. The third property minimizes reordering delay by optimizing for simultaneous arrival of a message timestamp and its corresponding barrier timestamp.

To maintain the second property, we store event timestamps as \textit{offsets} relative to local physical clock on end hosts and switches. When sending timestamps to the network, we add current clock time back to the offset.
This effectively uses physical clock to measure \textit{relative time} instead of absolute time, so the clock skew would not accumulate. Physical clocks driven by crystal oscillators and CPU instruction counters are accurate enough to measure short intervals, because their relative error is less than $10^{-5}$~\cite{corbett2013spanner} and the granularity is at nanosecond level to ensure each packet has a unique timestamp.
It worth noting that Sec.~\ref{sec:ideal} stores a timestamp barrier as an \textit{absolute value} to represent a slacked bound, while this subsection stores event timestamps as \textit{relative offsets} to represent unbiased estimations.

For the third property, we determine new event timestamp on end hosts according to feedback information from the network.
The synchronized timestamp from the network may be smaller than the last assigned event timestamp $T_{last}$. To prevent the event timestamps from decreasing, we track $T_{last}$ and let the new event timestamp to be the maximum of $T_{last} + 1$ and synchronized timestamp, as shown in Algorithm~\ref{alg:clock}. This effectively ``stalls'' the event timestamp until the synchronized timestamp catches up with it.

\begin{algorithm}[t]
 \DontPrintSemicolon
 \textbf{States:} Clock adjustment offset $off$\;
 \qquad Last assigned event timestamp $T_{last}$\;
 \qquad Local physical clock, fetched via $time()$\;
 \textbf{Event processing:}\;
 \If{received sync timestamp $T_{sync}$}{
 	$off \leftarrow T_{sync} - time()$\;
 }
 \ElseIf{a total-ordered event occurs}{
    EventTS $\leftarrow \max (T_{last} + 1, time() + off)$\;
    $T_{last} \leftarrow$ EventTS\;
 }
 \caption{Clock adjustment and event timestamp assignment on each end host.}
 \label{alg:clock}
\end{algorithm}


\begin{figure*}[t]
\centering
	\subfloat[Forward aggregation (max).\label{fig:minimax_max}]
	{\includegraphics[width=.33\textwidth]{images/minimax_example_1.pdf}}
	%\hspace{0.08\textwidth}
	\subfloat[Backward aggregation (min) and RTT compensation.\label{fig:minimax_min}]
	{\includegraphics[width=.33\textwidth]{images/minimax_example_2.pdf}}
	%\hspace{0.08\textwidth}
	\subfloat[Reorder delay after synchronization.\label{fig:minimax_result}]
	{\includegraphics[width=.3\textwidth]{images/minimax_example_3.pdf}}

\caption{Illustration of minimax clock synchronization.}
\label{fig:minimax}
\vspace{-1.2em}
\end{figure*}

To derive the timestamp feedback in the network, we need to estimate the delay from end-host senders to end-host receivers.
One-way delay is hard to measure, therefore we use round-trip time (RTT) instead.
Each end-host will add RTT to received feedback timestamp, just as illustrated in Figure~\ref{fig:minimax_min}.
When considering switches, one-hop RTT will be measured and compensated for feedback by each switch.

The challenge is that there are multiple senders and receivers, so we must aggregate timestamps from different hosts.
Let \texttt{FwdFunc} aggregate sender timestamps $T_i, i \in \mathcal{S}$ and \texttt{BackFunc} aggregate feedbacks from receivers $j \in \mathcal{R}$.
To simplify discussion, assume $d_{ij}$ is a fixed (but not measurable) one-way delay from sender $i$ to a receiver $j$, $r_{ji}$ is the fixed delay from receiver $j$ to sender $i$, and $RTT_{ij} = d_{ij} + r_{ji}$ is measurable. The new event timestamp $T'_k$ on a sender $k \in \mathcal{S}$ is:

\begin{equation*}
\begin{aligned}
T'_k & = \texttt{BackFunc} \{ \texttt{FwdFunc} \{ T_i - d_{ij} \} - r_{jk} + RTT_{kj} \} \\
     & = \texttt{BackFunc} \{ \texttt{FwdFunc} \{ T_i - d_{ij} \} + d_{kj} \}
\end{aligned}
\end{equation*}

On one hand, because timestamps must grow monotonically, all clocks in the system should catch up with the clock with highest timestamp.
To this end, $T'_k$ needs to reflect the maximum of sender timestamps $T_i$. Because $T_i, \forall i$ only appears in \texttt{FwdFunc}, \texttt{FwdFunc} needs to be $max$.

On the other hand, to ensure the second and fourth property of event timestamps, $\Sigma T_k, \forall k$ should converge. Choosing \texttt{BackFunc} as $min$ and \texttt{FwdFunc} as $max$ guarantees that each timestamp converge after one RTT for any initial assignment of $T_k$, \textit{i.e.}, $\forall k, T'_k = T''_k$. This self-stabilization property is proved in Appendix \ref{appx:minimax}.

Figure~\ref{fig:minimax} is a simple example of minimax synchronization.
The network consists of only two senders and two receivers.
In Figure~\ref{fig:minimax_max}, both senders assign timestamp to $0$ at the very beginning (equivalent to physical clock synchronized).
Based on linear time assumption, both receivers choose the maximum timestamp as feedback.
In Figure~\ref{fig:minimax_min}, both senders add RTT to feedback and synchronize to the minimum feedback timestamp.
Figure~\ref{fig:minimax_result} shows that compared to the original timestamp assignment, reorder delay declines after minimax synchronization.

Finally, to reduce the message complexity of clock synchronization, we use network switches to aggregate the $min$ and $max$ timestamps hierarchically, as shown in Algorithm~\ref{alg:minimax}.

\setlength{\textfloatsep}{0em}
\begin{algorithm}[t]
 \DontPrintSemicolon
 \textbf{States:} Per-port max timestamp $max_i, i \in \mathcal{I}$\;
 	\qquad Per-port min timestamp feedback $min_e, e \in \mathcal{E}$\;
    \qquad RTT estimation per egress link, $RTT_e, e \in \mathcal{E}$\;
    \qquad Received RTT probe request, $rttreq_i, i \in \mathcal{I}$\;
 	\qquad Local physical clock, fetched via $time()$\;
 \textbf{Event processing:}\\
 \If{send beacon packet to port $P$}{
    piggyback the following with the beacon packet:\\
 	max timestamp $T_{max}$:\\
    \quad $T_{max} = \max \{ max_i | i \in \mathcal{I} \wedge i \rightarrow P \} + time()$\;
    min timestamp $T_{min}$:\\
    \eIf{$\exists e, e \in \mathcal{E} \wedge P \rightarrow e$} {
        $T_{min} = \min \{ min_e | e \in \mathcal{E} \wedge P \rightarrow e \} + time()$\;
    }{
    	$T_{min} = T_{max}$\;
    }
    RTT probe request $rtt_{req}$:\\
    \quad $rtt_{req} = time()$\;
    RTT probe response $rtt_{ack}$:\\
    \quad $rtt_{ack} = time() - rttreq_P$\;
 }
 \ElseIf{received beacon packet from port $P$}{
    extract $T_{max}, T_{min}, rtt_{req}, rtt_{ack}$ from beacon\;
    $RTT_P \leftarrow \alpha \cdot (time() - rtt_{ack}) + (1 - \alpha) \cdot RTT_P$\;
 	$max_P \leftarrow T_{max} - time()$\;
    $min_P \leftarrow T_{min} + RTT_P - time()$\;
    $rttreq_P \leftarrow time() - rtt_{req}$\;
 }
 \caption{Minimax clock synchronization on each network switch and end host (treated as a single-port switch).}
 \label{alg:minimax}
\end{algorithm}


%One may ask why we use physical clock instead of Lamport clock (cite) or vector clock (cite) to measure intervals. A clock measures an interval by counting repetitive events. A good clock source should generate a roughly same number of repetitive events when our measurement object is stable, as well have a granularity finer than the intervals to measure.
%If we use packet counter as clock source, the RTT measurement will vary with network throughput. If we use received timestamps as clock source, it is unable to measure a tiny interval if no packet is received during the interval. Physical clocks driven by crystal oscillators and CPU instruction counters are accurate enough to measure short intervals, because their relative error is less than $10^{-5}$ (cite) and the granularity is as fine as a nanosecond.

\subsection{Reliable \sys in Lossy Networks}
\label{sec:lossy}

Above design only provides ordered but unreliable delivery.
In practice, network links typically experience one packet corruption every $10^5$ \texttildelow $10^7$ packets~\cite{zhuo2017understanding}.
Additionally, some devices or network links may have congestion or bugs to incur higher packet loss possibility~\cite{guo2015pingmesh}.
\sys detects packet loss in the network and mark barrier timestamps with a \textit{loss encountered} flag.
The original barrier timestamp is renamed to \textit{unreliable barrier}.
For applications requiring reliable ordered delivery, it can process the messages below unreliable barrier if the loss encountered flag is off.
When a loss is encountered, the receiver needs to wait for the senders to retransmit the lost messages.
\sys provides an \textit{ACK barrier} to indicate that all messages with timestamps below that barrier have been received by their desired recipients.

\setlength{\textfloatsep}{1em}
\begin{algorithm}[t]
 \DontPrintSemicolon
 \textbf{States:} Unreliable barrier $B_i, i \in \mathcal{I}$\;
 	\qquad ACK barrier $A_i, i \in \mathcal{I}$\;
 	\qquad Last-hop loss encountered flag $LE_i, i \in \mathcal{I}$\;
    \qquad Received loss encountered flag $RLE_i, i \in \mathcal{I}$\;
    \qquad Unreliable barrier at last loss $LB_i, i \in \mathcal{I}$\;
    \qquad Last loss counter $LC_i, i \in \mathcal{I}$\;
    \qquad Current loss counter, fetched via $C(i), i \in \mathcal{I}$\;
 \textbf{Event processing:}\\
 \If{\textnormal{Receive beacon packet from ingress $i$}}{
 	$RLE_i \leftarrow$ loss encounter flag in the packet\;
    $B_i \leftarrow$ unreliable barrier in the packet\;
    \If{$LC_i \neq C(i)$}{
    	$LE_i \leftarrow \textnormal{True}$\;
        $LB_i \leftarrow B_i$\;
        $LC_i \leftarrow C(i)$\;
    }
    $A_i \leftarrow$ ACK barrier in the packet\;
	\If{$A_i > LB_i \wedge LC_i = C(i)$}{
    	$LE_i \leftarrow \textnormal{False}$\;
    }
 }
 \ElseIf{\textnormal{Send beacon packet to egress $E$}}{
    Unreliable barrier $\leftarrow \min \{ B_i | i \in \mathcal{I} \wedge i \rightarrow E \}$\;
    ACK barrier $\leftarrow \min \{ A_i | i \in \mathcal{I} \wedge i \rightarrow E \}$\;
    Loss encountered flag $\leftarrow \exists i \in \mathcal{I} \wedge (i \rightarrow E) \wedge (LE_i \vee RLE_i)$\;
 }
 \caption{Hop-by-hop loss detection in network switches.}
 \label{alg:loss-detection}
\end{algorithm}

To detect packet loss in the network, we leverage per-port packet drop counter in commodity NICs and network switches.
If the packet drop counter of an ingress port changes, we know that a packet is lost during current interval and set the \textit{loss encountered} flag of the ingress port.
As a justification, our actual measurements find that the number of end-to-end packet losses agree with the packet drop counters in our test environment.


\begin{figure}[t]
\centering
\includegraphics[width=0.45\textwidth]{images/loss_detection.pdf}
\caption{Loss detection, recovery and ACK barriers.}
\label{fig:ack-barrier}
\vspace{-0.4em}
\end{figure}



Similar to TCP, packets have sequence numbers independent of timestamps.
The receiver responds ACK to the sender via non-timestamped packets.
For each connection, the sender maintains a \textit{minimum unacknowledged timestamp}, detect packet loss via duplicate ACKs or timeout, then retransmit the lost packets with the same timestamp (Figure~\ref{fig:ack-barrier}).
Each sender maintains an unacknowledged timestamp barrier (\textit{ACK barrier}), which is the minimum of minimum unacknowledged timestamps among all connections. Messages with timestamps below the ACK barrier are guaranteed to be received by all recipients.

ACK barriers are aggregated hierarchically in network switches.
ACK barrier also \textit{clears} the loss encountered flag of an ingress port.
As shown in Algorithm~\ref{alg:loss-detection}, the switch records the unreliable barrier at the time of marking the loss encountered flag. Then the switch waits for the ACK barrier to grow above this recorded unreliable barrier. If no packet loss occurred during this interval, the loss encountered flag can be cleared.


\subsection{Fault Tolerance and Incremental Deployment}
\label{sec:incremental}

Switches and end hosts detect network link or neighbor node failure when no beacons are received within a timeout interval.
Upon detection of failure, it will install a firewall rule to block data packets from the corresponding port and excludes the port from barrier aggregation and clock synchronization.
As an effect, the barriers are temporarily stalled during the timeout interval, and proceed to increase after the failure port is excluded.
In terms of CAP theorem~\cite{brewer2000towards}, we choose availability and partition tolerance over consistency between the partitions.
When the link or neighbor node recovers, it needs to rejoin the \sys system like a newcomer.

When an end host or switch needs to join a \sys system, it first initializes a \sys system by itself (all the timestamps are initialized arbitrarily). Therefore, the joining process is reduced to connecting two existing \sys systems with a new network link.

When merging \sys system $A$ with system $B$ via link $L$, the ingress ports from $L$ initially block all data packets and only allow beacon packets carrying barriers and clock synchronizations. 
When $A$ receives a beacon packet with barrier timestamp greater than or equal to the barrier timestamps of all L's possible egress ports, the ingress port from $L$ is counted in and the data packets are allowed to pass through.
If received barrier timestamp is smaller than one of L's possible egress port, it will be ignored.
If $A$ initially has higher timestamps than $B$, minimax clock synchronization makes $B$ catch up with $A$ within one RTT.
$B$ performs similarly and independently.
Consequently, in networks with stable link delay and clock speed, only one end-to-end RTT is required to recover from failure, add new hosts, switches or links, or merge two existing \sys systems.

\iffalse
\subsection{Inter-DC Topology}
\label{sec:inter-dc}


\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{images/inter-DC.pdf}
\caption{Partition an inter-DC network to multiple shards. Each color indicates a shard composed of one DC and the virtual routing paths originating from the DC.}
\label{fig:inter-dc}
\end{figure}



Intra-datacenter (intra-DC) routing paths are typically loop-free (Sec.~\ref{sec:dcn}), leading to efficient hierarchical aggregation. However, inter-DC routing paths are typically not loop-free. Fortunately, there is typically no ``hairpin'' traffic in inter-DC routing tables: packets passing through DC $A$ will never pass through some other DC $B$ and come back to $A$.

We partition the inter-DC network to $N$ logical shards, where $N$ is the number of DCs. Each shard is composed of one unique DC and all inter-DC traffic originating from the DC. Therefore, each inter-DC link is logically split to $N$ logical links, each carrying traffic originating from one DC.
The partitioning can be implemented by tagging each packet with an origin DC ID.
In this way, each switch in inter-DC WAN is logically split into $N$ switches, and at most $N$ beacon messages per inter-DC link are needed in each beacon interval.
Because the routes in each shard originate from a same DC, they form a DAG and are loop-free by our definition.\RED{(confused, loop-free logical shard leads to what?)}
Note that loop-free only affects liveness and is not required by correctness, therefore temporary routing loops will only stall message delivery until the loop disappears.

Typically, transit traffic of a DC only goes through border switches, analogous to ``international zones'' in an airport. If this is the case, we can handle intra-DC traffic as if no other DCs are present. For an incoming packet from another DC, the border switch removes the origin DC tag; for an outgoing packet, the border switch adds the origin DC tag.
\fi
